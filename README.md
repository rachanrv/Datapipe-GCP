# Introduction to Airflow- Datapipes Via GCP

This repository contains the files needed for implementation of Cloud-based Database using Google BQ and Python via Airflow.

## Description
* Create a Cloud-based Database in Google BQ with elegant and dynamic pipeline using Apache Airflow in few easy steps.
* Apache Airflow provides many plug-and-play operators that are ready to execute your tasks on Google Cloud Platform and other cloud platforms.
* Rudimentary knowledge of GCP and Big Query(BQ) to successfully create and implement Data pipe/Airflow is required.

## Getting Started

### Executing program

* Upload the *Make.csv* file in raw files folder to cloud storage bucket to create a data pipeline.
* Execute the sql queries in Big Query folder after customisation in Google BQ for creation of all the Big Query objects (tables and usp).
* Upload Error-free python code in Python folder after customisation to the DAG folder after creation of Cloud Composer Environment in GCP.

# Authors

Rachan Vamsi Bhooshi, main author of this paper is a Data professional with vast Cloud Data Lakehouse experience in GCP, Big Query, Snowflake, and ELT.

He is a Graduate Research Specialist in Supply Chain Analytics with a Master of Sciences in Business Analytics and Project Management(BAPM) at the University of Connecticut.

Email: rachanvamsiidp9@gmail.com

LinkedIn: http://linkedin.com/in/rachan-vamsi-3b0266ba

Revi Nadesan, contributing author is an experienced and Data passionate professional in Data Lakehouse (MPP & SaaS Databases) with Data Engineering & Science (DM, AI, ML, DL & NPL).

Email: rnadesan2005@gmail.com

LinkedIn: https://www.linkedin.com/in/revi-nadesan/

Aamir Abbas, contributing author has a background in web development. With prior experience on the business and sales side of the business. He is currently transiting to development and has a keen interest in JavaScript-related technologies.

Email: aamir@aamirmabs.com

LinkedIn: https://www.linkedin.com/in/aamirmabs/



